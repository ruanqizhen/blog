---
title: "科普一下人工智能"
date: "2023-03-19"
tags: 
  - "码农札记"
  - "红尘静悟"
---

最近我看了一位科普作家关于人工智能（AI）的几期视频，并查看了视频下的留言。发现该作家和许多观众都对目前的 AI 技术存在一些严重的误解。因此，我想写一篇科普文章，尽可能消除读者对 AI 的误解。

这篇文章专为 AI 领域之外的读者而写，我会尽量使用最简单的语言来解释和回答一些我认为最容易被大众所误解的问题，例如：

- AI 的行为是否连其设计者都不能理解？

- AI 是否能表现出超出设计行为？

- AI 是否具有创造力？

- AI 是否能产生自我意识？

- AI 为什么最近才能力爆发？

## AI 发展历史

首先，让我们简单回顾一下 AI 的历史。AI 的提出已经有八十年了。在过去的 80 年中，AI 曾几度引发热潮，人们都热衷于畅想 AI 的美好未来，也曾担心 AI 会取代人类。每一次热潮都没有持续太久，人们很快就会发现 AI 的局限性，AI 的话题也会再次淡出人们的视野。

虽然我对 AI 早有兴趣，但正式开始系统学习 AI 还是在 2012 年左右。当时，已经不再流行 AI 这个概念，取而代之的是“机器学习”（ML）。理论上来说，机器学习是人工智能的一种实现方法，实际上，我估计主要是因为当时 AI 正处于冷静期，不那么受追捧，而 ML 显得更为时尚、引人注目。

机器学习包含很多算法，例如朴素贝叶斯、决策树等等。你可能没有听说过这些算法，但你一定听说过神经网络。尽管现在神经网络占据了主导地位，但在我学习 AI 那会，它并没有受到多数专家的青睐。这主要是因为神经网络也是一种有些历史的算法了，但在最初几十年的发展历程中，并没有表现出特别优异的性能。在图像领域中，它无法与支持向量机相媲美；在自然语言处理领域中，它又不如隐马尔可夫模型。当时很多人认为神经网络可能没有什么前途了。

然而，神经网络最终还是崛起了。现在所有引人注目的 AI 项目都是基于神经网络的，因此有必要简要介绍一下神经网络的工作原理。在下文中提到 AI 时，默认指代的就是神经网络模型。

## 神经网络的原理

我们可以把需要解决的问题抽象为一个函数。比如，一个用来计算乘法的函数，它的输入是两个数，x 和 y，输出是另一个数；用于聊天的函数，它的输入是一些文字，输出是另一些文字；用于绘图的函数，输入是一些文字，输出是一张图片。

有些函数是非常简单的，比如乘法函数，根本用不着神经网路，有更高效简洁的方法可以解决。神经网络主要应用于那些特别复杂的问题，比如通过文字绘图的函数，人们没办法直接为这类函数提炼出一个简单直观的数学公式。但问题还是可以解决：如果读者听说过“级数”这个概念，可能已经知道了，任何一个函数，都可以被看作是很多很多其它函数的叠加；如果没听说过也不要紧，只要记住这个结论就可以了：如果遇到一个极其复杂的函数，我们可以把它拆解成许许多多简单的小函数，再让这些小函数的结果叠加在一起，恰好可以实现复杂函数的功能。

假设存在一个复杂的函数，输入 x 会产生输出 y，可以将其分解为多个简单函数的和，形如：

Y = (w0\*x + b0) + (w1\*x + b1) + (w2\*x + b2) + …….. + (wn\*x + bn)

其中每个 (w\*x + b) 的部分就是神经网络中的一个节点。而 w0 ~ wn 和 b0 ~ bn 则是神经网络中的参数。这些参数的值不是人为设定的，而是通过神经网络的训练过程去找到的。实际应用中的神经网络会比上述例子更加复杂，比如每个节点的函数会是更复杂一点的非线性函数，会有更多的层次等，但原理依然相同。

为了找到最适合的参数值，首先需要为模型构建一个训练集，例如，对于生成图片的模型，每个训练条目包含两部分：输入的文字和输出的图片。模型首先使用输入的文字作为输入，使用当前的参数生成一张图片，然后将其与目标图片进行比较，提取差异。根据差异的大小，调整网络中的每个参数，使其产生更接近目标图片的结果。不断重复这一过程，直到模型产生的图片与目标图片相似为止。

这个过程可以看作是模拟数据集或拟合目标函数的过程。

## AI 的行为是否连其设计者都不能理解？

当我训练好一个神经网络之后，我是否能够知道网络中每个参数的值呢？是的，只要想要，我就可以知道。从上文所述的模型训练过程可以看出，这是一个机械的数学计算过程。如果给定相同的初始值和训练数据，重复相同的训练过程，必然会得到相同的参数。但作为模型设计者，我实际上并不关心每个参数的具体值，因此我不会去查看每个参数的值。

这就造成了很多人对于神经网络的一个误解，他们会说：原来你都不知道神经网络里具体的数据啊，肯定更不知道每个参数的含义啦，你自己设计的神经网络你自己都理解不了，看来人工智能要摆脱人类控制了。

这种说法类似于对一个火箭设计者说：你连每个燃料分子燃烧的位置都不知道，你怎么能理解你设计的火箭呢？但实际上，设计火箭并不需要了解每个分子的情况，只要知道所有燃料作为一个整体能否提供足够的动力就可以了。同样的，神经网络的设计者也不需要，更不应该去纠结网络中每个参数的值，设计者应该去了解的是当所有参数作为一个整体时，能否拟合目标函数。

现在读者应该可以看出来了，所谓 AI 设计者不了解自己设计的 AI，纯粹是一种误解。其实，只要换位思考一下就知道这种误解的不合逻辑之处：读者自己是否理解 AI 的工作原理？如果理解不那么深刻，那么你能够设计出一个 AI 模型吗？如果你因为不完全了解 AI 而无法设计一个 AI 模型，又怎么能相信别人会在不了解 AI 的情况下就设计出了一个好用的 AI 模型呢？

## AI 是否能表现出超出设计的行为？

神经网络的学习过程本质上是在尝试最大限度地拟合训练数据集，它通过不断调整网络中的参数来达到这个目的。然而，这也意味着神经网路被训练到极致，最多也就是完美得模仿了训练数据集。

上文用图片生成做过示例，再用文字生成为例讲解一下 AI 如何工作。大家可能觉得AI聊天写作文等功能挺神奇的，但实际上，文字的生成也可以被看作是一个函数：这个函数的输入是一些文字，输出是下一个要产生的字。

现在的大语言模型常常会使用网络上能够收集到的所有文章、对话作为训练集。这样，训练出的模型，就可以模仿网络上的文字。比如：把“我爱祖”三个子输入给训练好的 AI 模型，AI 模型由于在训练集中看到了大量“我爱祖国”这样的组合，于是就会输出一个“国”字。“我喜欢祖国”可能并不常见，但模型在训练集中会发现有大量的情况“喜欢”和“爱”这两个词是可以互换的。 所以，如果把“我喜欢祖”这四个字输入给模型，模型也会在这里输出一个“国”字。如果输入“小明：你喜欢祖冲之吗？AI：我喜欢祖”这几个字输入给模型，模型因为在大量的训练集中已经发现了，对话中的文字必须是上下文相关，所以会在这里输出一个“冲”字，再把“小明：你喜欢祖冲之吗？AI：我喜欢祖冲”几个字输入给模型，模型会再返回一个“之”字。

训练好的模型，可以写文章，可以解释法律、医学等问题，但无论它看上去多么令人惊叹，也只是在忠实的按照设计目标，去模仿网上的文章与对话。它无法超越设计者给它限定的范围。

同时这也可以解释为什么现在的大语言模型有时会一本正经的胡说八道：在遇到训练集中没有的问题时，它会提取一些最相关的内容进行拼凑，但它无法判断拼凑出来的东西是否正确。

## AI 有没有创造力？

这要看你如何定义创造力了。一个 AI 在学习了猫头，狗身的照片之后，可以画出一张猫头狗身拼接在一起的照片。如果你觉得这算是创造力，那么AI确实具有了一定的创造力。

如果你觉得不算，只有 AI 画出了训练集中没有的东西才算创造，比如训练集中只有猫和狗，但模型却能画出一匹马。如果你这样定义创造力的话，那么 AI 没有创造力，AI 无法输出任何训练集中没有的东西，它最多只能把训练集中的内容进行整理拼接。

## AI 能不能产生自我意识？

就目前AI的技术来说，AI 是绝不会产生自我意识的。目前训练 AI 的方法就是让 AI 去模拟训练集中的输入输出。我们可以脑洞一下，如果有一个 AI 模型突然就有自我意识了，训练程序让它画猫，它决定放飞自我，画了一只鸡。结果会怎样呢？结果是：因为它没有达到设计目标，这个模型被销毁了。

以目前的 AI 技术而言，AI 本质上与螺丝刀、扳手等工具没有什么区别，都是由人类设计和使用的工具：人类怎么操作它，它就怎么工作，除此之外无法做任何事情。任何人类不知道如何解决的问题，AI 也同样解决不了。比如，人类如果不知道如何证明黎曼猜想，AI 也无法自己证明。

总有人觉得 AI 参数这么多，结构这么复杂，说不定它自己突然灵光一现，就产生了自我意识呢？这个想法，就有点类似于，把组成生命的各种化学物质放到一个瓶子里，然后晃一晃瓶子，就指望里面的分子自己重新排列，构建一个生命体出来。我们有时候会有一些美好的愿望，希望自己并不了解的东西可以自发的产生魔法和奇迹。但是最终这些愿望基本都会落空。

## 为什么神经网络这么强大，却直到最近才功能爆发？

AI 最近爆火起来，是它在绘画，文字生成等领域都展现出了前所未有的好结果。那么为什么它之前没有，而现在突然就强大了呢？

早期神经网络主要是全链接神经网路，也就是网络中的每个神经节点都和下一层的所有神经节点相连。理论上，一层这样的神经网络就可以拟合任何函数，但在实际应用中，它始终面临着三个未能解决的问题：

- 训练效率差，尤其是大型，多层神经网络，很多情况下根本无法训练。

- 硬件不能提供足够的计算资源。要拟合复杂的问题，少量神经节点肯定是不够的，节点必然越多越好。

- 缺少训练集，标注训练集是非常贵的。

近些年，以上几个问题都得到了实质性的改进。首先是算法和网络架构的创新解决了模型训练的问题。使用一些特殊的网络结构（如卷积网络、残差网络和注意力机制等）代替全链接网络，可以提高模型的训练效率和表现。这些结构可以构建上百层甚至更深的网络结构，“深度学习”由此得名。现在，那些最为知名的大型模型都是基于“变形金刚”这种链接结构构建的。

要解决更复杂的问题，必然需要更多的神经元。在硬件方面，GPU的出现，为神经网络提供了足够的计算资源。

人为标注的训练集依然十分昂贵，但是这些年互联网的蓬勃发展使得网络上的文字和图像内容极其丰富，这些内容都可以被用来做训练。

于是乎，解决了以上几个问题的 AI 开始了新一轮的大爆发。作为相关从业人员，我希望这一次的热度可以坚持更长的时间。

![](untitled.png)
