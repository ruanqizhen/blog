---
title: "科普一下人工智能"
date: "2023-03-19"
tags: 
  - "码农札记"
---

最近，我看了一位科普作家关于人工智能（AI）的几期视频，并留意到视频下的留言区，发现许多人对当前的 AI 技术存在一些普遍的误解。因此，我决定写这篇科普文章，希望帮助读者澄清一些常见的错误认知。

这篇文章专为非 AI 领域的读者而写，我会尽量用简单、易懂的语言解答几个广受关注的问题，例如：
- AI 的行为是否连其设计者都不能理解？
- AI 是否能表现出超出设计行为？
- AI 是否具有创造力？
- AI 是否能产生自我意识？
- AI 为什么最近才能力爆发？

## AI 发展历史

首先，让我们简要回顾一下人工智能的历史。人工智能的概念最早可以追溯到八十年前。从那时起，AI 的发展经历了数次热潮和低谷。每次技术突破都会让人们对 AI 的未来充满期待，同时也伴随着担忧，比如 AI 是否会威胁人类的工作岗位或控制权。然而，每次热潮过后，技术的局限性又让热情逐渐冷却，直到下一次突破带来新一轮的关注。

虽然我对人工智能的兴趣由来已久，但真正开始系统学习是在 2012 年左右。当时，"人工智能"这个词已经不再是热门话题，取而代之的是“机器学习”（ML）。严格来说，机器学习是人工智能的一种实现方法，实际上，我估计主要是因为当时 AI 正处于冷静期，不那么受追捧，而 ML 显得更为时尚、引人注目。这种术语的变化也反映了公众关注点的转移。

机器学习包含多种算法，比如朴素贝叶斯、决策树等。这些名字可能听起来陌生，但你可能听说过其中的一种关键技术：神经网络。在我开始学习 AI 的时候，神经网络并不受主流认可。虽然它的概念提出已有几十年历史，但当时在许多应用领域的表现并不突出，比如在图像识别中不如支持向量机，在自然语言处理中也逊色于隐马尔可夫模型。因此，很多人认为神经网络前景有限。

然而，技术的进步改变了一切。近年来，神经网络以惊人的速度崛起，成为 AI 的核心技术。如今，几乎所有引人瞩目的 AI 项目都依赖神经网络，因此有必要简要介绍其工作原理。接下来，本文提到的 AI 默认指代基于神经网络的模型。

## 神经网络的原理

要理解神经网络，可以把它看作是一个用来解决复杂问题的工具。我们可以将问题抽象为一个函数：

- 比如，一个用来计算乘法的函数，它的输入是两个数，输出是它们的乘积；
- 一个聊天生成的函数，输入是一些文字，输出是相应的回复；
- 一个绘图的函数，输入是一些文字，输出是一幅图像。

对于简单的问题，比如乘法函数，我们可以直接使用已有的高效算法来解决，不需要神经网络。

神经网络主要应用于那些特别复杂的问题，比如通过文字绘图的函数，人们没办法直接为这类函数提炼出一个简单直观的数学公式。但问题还是可以解决：如果读者听说过“级数”这个概念，可能已经知道了，任何一个函数，都可以被看作是很多很多其它函数的叠加；如果没听说过也不要紧，只要记住这个结论就可以了：如果遇到一个极其复杂的函数，我们可以把它拆解成许许多多简单的小函数，再让这些小函数的结果叠加在一起，恰好可以实现复杂函数的功能。

假设我们有一个复杂的函数，它的输入是 x，输出是 y，可以将其表示为多个简单函数的叠加：

Y = (w0\*x + b0) + (w1\*x + b1) + (w2\*x + b2) + ... + (wn\*x + bn)

其中每个 (w\*x + b) 的部分就是神经网络中的一个节点。而 w0 ~ wn 和 b0 ~ bn 则是神经网络中的参数。这些参数的值不是人为设定的，而是通过神经网络的训练过程去找到的。实际应用中的神经网络会比上述例子更加复杂，比如每个节点的函数会是更复杂一点的非线性函数，会有更多的层次等，但原理依然相同。

为了找到最适合的参数值，首先需要为模型构建一个训练集，例如，对于生成图片的模型，每个训练条目包含两部分：输入的文字和输出的图片。模型首先使用输入的文字作为输入，使用当前的参数生成一张图片，然后将其与目标图片进行比较，提取差异。根据差异的大小，调整网络中的每个参数，使其产生更接近目标图片的结果。不断重复这一过程，直到模型产生的图片与目标图片相似为止。

这一过程被称为“训练”，可以看作是模型模拟数据集或拟合目标函数的过程。通过不断优化，神经网络能够从数据中学习到复杂问题的解决方法。

## AI 的行为是否连其设计者都不能理解？

当我训练好一个神经网络之后，我是否能够知道网络中每个参数的值呢？是的，只要想要，我就可以知道。从上文所述的模型训练过程可以看出，这是一个机械的数学计算过程。如果给定相同的初始值和训练数据，重复相同的训练过程，必然会得到相同的参数。但作为模型设计者，我实际上并不关心每个参数的具体值，因此我不会去查看每个参数的值。

这就造成了很多人对于神经网络的一个误解，他们会说：原来你都不知道神经网络里具体的数据啊，肯定更不知道每个参数的含义啦，你自己设计的神经网络你自己都理解不了，看来人工智能要摆脱人类控制了。

这种说法类似于对一个火箭设计者说：你连每个燃料分子燃烧的位置都不知道，你怎么能理解你设计的火箭呢？但实际上，设计火箭并不需要了解每个分子的情况，只要知道所有燃料作为一个整体能否提供足够的动力就可以了。同样的，神经网络的设计者也不需要，更不应该去纠结网络中每个参数的值，设计者应该去了解的是当所有参数作为一个整体时，能否拟合目标函数。

现在读者应该可以看出来了，所谓 AI 设计者不了解自己设计的 AI，纯粹是一种误解。其实，只要换位思考一下就知道这种误解的不合逻辑之处：读者自己是否理解 AI 的工作原理？如果理解不那么深刻，那么你能够设计出一个 AI 模型吗？如果你因为不完全了解 AI 而无法设计一个 AI 模型，又怎么能相信别人会在不了解 AI 的情况下就设计出了一个好用的 AI 模型呢？

## AI 是否能表现出超出设计的行为？

神经网络的学习过程本质上是在尝试最大限度地拟合训练数据集，它通过不断调整网络中的参数来达到这个目的。然而，这也意味着神经网路被训练到极致，最多也就是完美得模仿了训练数据集。

上文用图片生成做过示例，再用文字生成为例讲解一下 AI 如何工作。大家可能觉得AI聊天写作文等功能挺神奇的，但实际上，文字的生成也可以被看作是一个函数：这个函数的输入是一些文字，输出是下一个要产生的字。

现在的大语言模型常常会使用网络上能够收集到的所有文章、对话作为训练集。这样，训练出的模型，就可以模仿网络上的文字。比如：把“我爱祖”三个子输入给训练好的 AI 模型，AI 模型由于在训练集中看到了大量“我爱祖国”这样的组合，于是就会输出一个“国”字。“我喜欢祖国”可能并不常见，但模型在训练集中会发现有大量的情况“喜欢”和“爱”这两个词是可以互换的。 所以，如果把“我喜欢祖”这四个字输入给模型，模型也会在这里输出一个“国”字。如果输入“小明：你喜欢祖冲之吗？AI：我喜欢祖”这几个字输入给模型，模型因为在大量的训练集中已经发现了，对话中的文字必须是上下文相关，所以会在这里输出一个“冲”字，再把“小明：你喜欢祖冲之吗？AI：我喜欢祖冲”几个字输入给模型，模型会再返回一个“之”字。

训练好的模型，可以写文章，可以解释法律、医学等问题，但无论它看上去多么令人惊叹，也只是在忠实的按照设计目标，去模仿网上的文章与对话。它无法超越设计者给它限定的范围。

同时这也可以解释为什么现在的大语言模型有时会一本正经的胡说八道：在遇到训练集中没有的问题时，它会提取一些最相关的内容进行拼凑，但它无法判断拼凑出来的东西是否正确。

## AI 有没有创造力？

这要看你如何定义创造力了。一个 AI 在学习了猫头，狗身的照片之后，可以画出一张猫头狗身拼接在一起的照片。如果你觉得这算是创造力，那么AI确实具有了一定的创造力。

如果你觉得不算，只有 AI 画出了训练集中没有的东西才算创造，比如训练集中只有猫和狗，但模型却能画出一匹马。如果你这样定义创造力的话，那么 AI 没有创造力，AI 无法输出任何训练集中没有的东西，它最多只能把训练集中的内容进行整理拼接。

## AI 能不能产生自我意识？

就目前AI的技术来说，AI 是绝不会产生自我意识的。目前训练 AI 的方法就是让 AI 去模拟训练集中的输入输出。我们可以脑洞一下，如果有一个 AI 模型突然就有自我意识了，训练程序让它画猫，它决定放飞自我，画了一只鸡。结果会怎样呢？结果是：因为它没有达到设计目标，这个模型被销毁了。

以目前的 AI 技术而言，AI 本质上与螺丝刀、扳手等工具没有什么区别，都是由人类设计和使用的工具：人类怎么操作它，它就怎么工作，除此之外无法做任何事情。任何人类不知道如何解决的问题，AI 也同样解决不了。比如，人类如果不知道如何证明黎曼猜想，AI 也无法自己证明。

总有人觉得 AI 参数这么多，结构这么复杂，说不定它自己突然灵光一现，就产生了自我意识呢？这个想法，就有点类似于，把组成生命的各种化学物质放到一个瓶子里，然后晃一晃瓶子，就指望里面的分子自己重新排列，构建一个生命体出来。我们有时候会有一些美好的愿望，希望自己并不了解的东西可以自发的产生魔法和奇迹。但是最终这些愿望基本都会落空。

## 为什么神经网络这么强大，却直到最近才功能爆发？

AI 最近爆火起来，是它在绘画，文字生成等领域都展现出了前所未有的好结果。那么为什么它之前没有，而现在突然就强大了呢？

早期神经网络主要是全链接神经网路，也就是网络中的每个神经节点都和下一层的所有神经节点相连。理论上，一层这样的神经网络就可以拟合任何函数，但在实际应用中，它始终面临着三个未能解决的问题：

- 训练效率差，尤其是大型，多层神经网络，很多情况下根本无法训练。

- 硬件不能提供足够的计算资源。要拟合复杂的问题，少量神经节点肯定是不够的，节点必然越多越好。

- 缺少训练集，标注训练集是非常贵的。

近些年，以上几个问题都得到了实质性的改进。首先是算法和网络架构的创新解决了模型训练的问题。使用一些特殊的网络结构（如卷积网络、残差网络和注意力机制等）代替全链接网络，可以提高模型的训练效率和表现。这些结构可以构建上百层甚至更深的网络结构，“深度学习”由此得名。现在，那些最为知名的大型模型都是基于“变形金刚”这种链接结构构建的。

要解决更复杂的问题，必然需要更多的神经元。在硬件方面，GPU的出现，为神经网络提供了足够的计算资源。

人为标注的训练集依然十分昂贵，但是这些年互联网的蓬勃发展使得网络上的文字和图像内容极其丰富，这些内容都可以被用来做训练。

于是乎，解决了以上几个问题的 AI 开始了新一轮的大爆发。作为相关从业人员，我希望这一次的热度可以坚持更长的时间。

![](untitled.png)
